<otsl><caption>Research Papers Database</caption><loc_153><loc_276><loc_430><loc_396><ched>Title<ched>Abstract<ched>Year<nl><fcel>Attention Is All You Need<fcel>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.<fcel>2017<nl><fcel>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<fcel>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.<fcel>2018<nl><fcel>GPT-4 Technical Report<fcel>We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers.<fcel>2023<nl></otsl>