<!-- Test case: Table with very long text content -->
<table border="1">
  <caption>Research Papers Database</caption>
  <thead>
    <tr>
      <th>Title</th>
      <th>Abstract</th>
      <th>Year</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Attention Is All You Need</td>
      <td>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.</td>
      <td>2017</td>
    </tr>
    <tr>
      <td>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</td>
      <td>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.</td>
      <td>2018</td>
    </tr>
    <tr>
      <td>GPT-4 Technical Report</td>
      <td>We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers.</td>
      <td>2023</td>
    </tr>
  </tbody>
</table>
